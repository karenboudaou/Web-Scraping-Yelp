{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** \n",
    "In this task, you will be performing sentiment analysis on Yelp reviews using the VADER sentiment analyzer. Here's what you need to do:\n",
    "\n",
    "1. **Load the Data**: Load the Yelp reviews from a CSV file (`yelp_reviews.csv`) into a pandas DataFrame.\n",
    "\n",
    "2. **Initialize the Sentiment Analyzer**: Set up the VADER sentiment analyzer to measure the sentiment of the reviews.\n",
    "\n",
    "3. **Create a Sentiment Function**: Write a function that calculates the positive, negative, neutral, and overall sentiment scores for each review using VADER.\n",
    "\n",
    "4. **Apply the Sentiment Analysis**: Apply this function to all the reviews in the dataset and add the sentiment scores as new columns to your DataFrame.\n",
    "\n",
    "5. **Save Your Work**: Save the updated DataFrame with the sentiment scores to a new CSV file called `yelp_reviews_vader.csv`.\n",
    "\n",
    "The goal is to analyze Yelp reviews for sentiment and store the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-01T19:55:07.810017Z"
    }
   },
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Load the Yelp reviews dataset \n",
    "reviews_df = pd.read_csv(\"/Users/kaykaydaou/Desktop/MMA/MMA WINTER 25/W1 TEXT ANALYTICS/Labs/lab 3/yelp_reviews.csv\")\n",
    "\n",
    "#Initialize VADER sentiment analyzer \n",
    "sia = SentimentIntensityAnalyzer() \n",
    "\n",
    "#Function to calculate VADER sentiment for each review\n",
    "def get_vader_sentiment(review):\n",
    "    #Calculate sentiment using sia.polarity_scores(review)\n",
    "    #Return the positive, negative, neutral, and compound sentiment scores\n",
    "    sentiment = sia.polarity_scores(review)  \n",
    "    return sentiment['pos'], sentiment['neg'], sentiment['neu'], sentiment['compound']  \n",
    "\n",
    "#Apply the VADER sentiment analysis to the 'Review' column\n",
    "#Using the DataFrame's 'apply' function to apply get_vader_sentiment to each review, then unpacking the scores into new columns ('pos', 'neg', 'neu', 'compound')\n",
    "reviews_df['pos'], reviews_df['neg'], reviews_df['neu'], reviews_df['compound'] = reviews_df['Review'].apply(lambda x: pd.Series(get_vader_sentiment(x))) \n",
    "\n",
    "#Save the updated DataFrame with the VADER sentiment scores to a CSV file \n",
    "# Ensure the file is named 'yelp_reviews_vader.csv'\n",
    "reviews_df.to_csv(\"yelp_reviews_vader.csv\", index=False)  # Replace None with the correct file name\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** In this task, you will be working on preprocessing text and performing sentiment analysis using custom rules and WordNet similarity. Here's what you need to do:\n",
    "\n",
    "1. **Preprocess the Text**: Implement a function to clean up the review text. This involves tokenizing the text, removing stopwords, and lemmatizing (reducing words to their base form).\n",
    "\n",
    "2. **Extract Phrases**: Write a function that identifies and extracts specific types of phrases from the text, based on rules like \"adjective followed by noun\" or \"adverb followed by verb.\"\n",
    "\n",
    "3. **Calculate Word Similarity**: Use WordNet to calculate the similarity between words. This will help in determining whether the extracted phrases are more similar to positive or negative words.\n",
    "\n",
    "4. **Analyze Sentiment**: For each review, analyze the sentiment by checking if the extracted phrases are more similar to positive or negative reference words. Then assign a score and label (positive/negative) to each review.\n",
    "\n",
    "5. **Apply Sentiment Analysis**: Apply your sentiment analysis function to all the reviews in the dataset and save the results (sentiment score and label) to a new CSV file.\n",
    "\n",
    "The goal is to preprocess the text, extract meaningful phrases, and determine the sentiment of each review using custom rules and word similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete Text Preprocessing and Sentiment Analysis Script for Students\n",
    "\n",
    "# TODO: Import necessary libraries (Hint: You'll need nltk for lemmatization and stop words, pandas for data handling, and WordNet for semantic similarity)\n",
    "\n",
    "# Initialize the lemmatizer and stop words list\n",
    "lemmatizer = None  # TODO: Initialize WordNetLemmatizer()\n",
    "stop_words = None  # TODO: Get the list of stopwords from nltk\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # TODO: Tokenize the text (Hint: Use word_tokenize)\n",
    "    tokens = None  # Replace None with tokenization code\n",
    "\n",
    "    # TODO: Convert tokens to lowercase and remove non-alphanumeric characters\n",
    "    # Filter out stop words and lemmatize the remaining tokens\n",
    "    tokens = None  # Replace None with list comprehension for lowercasing, filtering, and lemmatization\n",
    "    return tokens\n",
    "\n",
    "def extract_phrases(tokens):\n",
    "    \"\"\"\n",
    "    Extracts phrases from the tokens based on the provided rules.\n",
    "    \"\"\"\n",
    "    # TODO: POS-tag the tokens (Hint: Use pos_tag)\n",
    "    pos_tagged = None  # Replace None with POS tagging code\n",
    "\n",
    "    phrases = []\n",
    "    # TODO: Implement rules to extract phrases (use the given rules 1-5 as hints)\n",
    "    for i in range(len(pos_tagged) - 2):\n",
    "        word1, tag1 = pos_tagged[i]\n",
    "        word2, tag2 = pos_tagged[i + 1]\n",
    "        word3, tag3 = pos_tagged[i + 2] if i + 2 < len(pos_tagged) else None\n",
    "\n",
    "        # Rule 1: JJ followed by NN or NNS\n",
    "        if tag1.startswith('JJ') and (tag2 == 'NN' or tag2 == 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 2: RB, RBR, or RBS followed by JJ, not followed by NN or NNS\n",
    "        elif (tag1 == 'RB' or tag1 == 'RBR' or tag1 == 'RBS') and tag2.startswith('JJ') and (tag3 != 'NN' and tag3 != 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 3: JJ followed by JJ, not followed by NN or NNS\n",
    "        elif tag1.startswith('JJ') and tag2.startswith('JJ') and (tag3 != 'NN' and tag3 != 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 4: NN or NNS followed by JJ, not followed by NN or NNS\n",
    "        elif (tag1 == 'NN' or tag1 == 'NNS') and tag2.startswith('JJ') and (tag3 != 'NN' and tag3 != 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 5: RB, RBR, or RBS followed by VB, VBD, VBN, or VBG\n",
    "        elif (tag1 == 'RB' or tag1 == 'RBR' or tag1 == 'RBS') and (tag2 == 'VB' or tag2 == 'VBD' or tag2 == 'VBN' or tag2 == 'VBG'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "\n",
    "    return phrases\n",
    "\n",
    "def wordnet_similarity(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two words using WordNet.\n",
    "    \"\"\"\n",
    "    # TODO: Get synsets for the two words using wn.synsets\n",
    "    synsets1 = None  # Replace None with code to get synsets for word1\n",
    "    synsets2 = None  # Replace None with code to get synsets for word2\n",
    "\n",
    "    if synsets1 and synsets2:\n",
    "        # TODO: Calculate similarity using wn.wup_similarity\n",
    "        return None  # Replace None with the similarity calculation\n",
    "\n",
    "    return 0  # Return 0 if no similarity is found\n",
    "\n",
    "# Define positive and negative reference words\n",
    "positive_refs = [\"delicious\", \"tasty\", \"amazing\", \"great\", \"wonderful\", \"fantastic\", \"excellent\"]\n",
    "negative_refs = [\"disgusting\", \"bad\", \"terrible\", \"awful\", \"horrible\", \"inedible\", \"poor\"]\n",
    "\n",
    "def semantic_orientation(phrase):\n",
    "    \"\"\"\n",
    "    Calculates the semantic orientation of a phrase by comparing it with\n",
    "    multiple positive and negative reference words.\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the average similarity with positive and negative reference words\n",
    "    pos_score = None  # Replace None with similarity calculation for positive reference words\n",
    "    neg_score = None  # Replace None with similarity calculation for negative reference words\n",
    "\n",
    "    # Return the difference to get the orientation score\n",
    "    return pos_score - neg_score\n",
    "\n",
    "def analyze_sentiment(document):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of a document based on its phrases' semantic orientation\n",
    "    and returns both the sentiment score and the sentiment label.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(document)\n",
    "    phrases = extract_phrases(tokens)\n",
    "    total_orientation = 0\n",
    "\n",
    "    for phrase in phrases:\n",
    "        total_orientation += semantic_orientation(phrase)\n",
    "\n",
    "    # TODO: Assign sentiment label based on total_orientation\n",
    "    sentiment_label = None  # Replace None with logic for assigning 'Positive' or 'Negative' label\n",
    "\n",
    "    return total_orientation, sentiment_label  # Return both the sentiment score and label\n",
    "\n",
    "# TODO: Load the dataset (Hint: Use pandas to read 'yelp_reviews.csv')\n",
    "data = None  # Replace None with pandas code to load the CSV file\n",
    "\n",
    "# TODO: Apply sentiment analysis to the 'Review' column and calculate both the score and label\n",
    "# Hint: Use the apply method with a lambda function to apply analyze_sentiment to each review\n",
    "data[['Sentiment Score', 'Sentiment Label']] = None  # Replace None with code to apply sentiment analysis\n",
    "\n",
    "# TODO: Save the results to a new CSV file (Hint: Use pandas to_csv method)\n",
    "data.to_csv(None, index=False)  # Replace None with the correct filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** In this task, you will preprocess Yelp reviews and calculate the Euclidean distances between them based on their term frequencies. Here's what you need to do:\n",
    "\n",
    "1. **Preprocess the Text**: Implement a function that cleans up the review text by removing punctuation, converting it to lowercase, tokenizing it, and removing common stopwords. You will also lemmatize the words to reduce them to their base form.\n",
    "\n",
    "2. **Apply Preprocessing**: Apply this text preprocessing function to each review in the dataset to prepare the text for analysis.\n",
    "\n",
    "3. **Convert to Term Matrix**: Convert the cleaned review text into a numerical format (term frequency matrix) using `CountVectorizer`, which will help you calculate distances between the reviews.\n",
    "\n",
    "4. **Calculate Euclidean Distances**: Use the term matrix to compute the Euclidean distances between each pair of reviews, which will help show how similar or different the reviews are from one another.\n",
    "\n",
    "5. **Create and Save a DataFrame**: Convert the distance results into a DataFrame so it’s easier to view and save this as a CSV file.\n",
    "\n",
    "The goal is to process the text data and then compute how close or far apart the reviews are from each other based on the words they contain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete Text Preprocessing and Distance Calculation Script for Students\n",
    "\n",
    "# TODO: Import necessary libraries (Hint: You'll need nltk for lemmatization and stop words, pandas for data handling, re for regular expressions, and sklearn for vectorization and distance calculations)\n",
    "\n",
    "# Step 1: Initialize the lemmatizer and stop words\n",
    "lemmatizer = None  # TODO: Initialize WordNetLemmatizer\n",
    "stop_words = None  # TODO: Get the set of English stop words using nltk\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, removing stopwords, lemmatizing, and cleaning punctuation.\n",
    "    \"\"\"\n",
    "    # TODO: Remove special characters (Hint: Use re.sub to remove anything that's not a letter or space)\n",
    "    text = None  # Replace None with text cleaning code\n",
    "\n",
    "    # TODO: Convert the text to lowercase\n",
    "    text = None  # Replace None with code to convert text to lowercase\n",
    "\n",
    "    # TODO: Tokenize the text (Hint: Use word_tokenize from nltk)\n",
    "    tokens = None  # Replace None with tokenization code\n",
    "\n",
    "    # TODO: Remove stopwords and lemmatize the tokens\n",
    "    tokens = None  # Replace None with list comprehension for lemmatization and stopword removal\n",
    "    \n",
    "    # Return the processed text as a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# TODO: Load the dataset (Hint: Use pandas to read 'yelp_reviews.csv')\n",
    "data = None  # Replace None with code to load the dataset\n",
    "\n",
    "# Step 3: Apply text preprocessing to the 'Review' column\n",
    "# TODO: Use the apply method to apply preprocess_text to each review\n",
    "data['Cleaned_Reviews'] = None  # Replace None with the code to apply preprocessing\n",
    "\n",
    "# Step 4: Convert documents to a term frequency matrix using CountVectorizer\n",
    "# TODO: Initialize the CountVectorizer and fit it to the 'Cleaned_Reviews' column\n",
    "vectorizer = None  # Replace None with CountVectorizer initialization\n",
    "term_matrix = None  # Replace None with the fit_transform code for the Cleaned_Reviews\n",
    "\n",
    "# Step 5: Calculate the pairwise Euclidean distances between documents\n",
    "# TODO: Use euclidean_distances from sklearn to calculate the distances between the term frequency vectors\n",
    "distances = None  # Replace None with the distance calculation code\n",
    "\n",
    "# Step 6: Convert the pairwise distances into a matrix format\n",
    "# Step 7: Convert the matrix into a DataFrame for easier viewing\n",
    "# TODO: Create a DataFrame from the distances matrix (Hint: Use pandas DataFrame and set the index and columns to data.index)\n",
    "distance_df = None  # Replace None with DataFrame creation code\n",
    "\n",
    "# Step 8: Save the DataFrame to a CSV file\n",
    "# TODO: Save the DataFrame to a CSV file (Hint: Use to_csv method)\n",
    "distance_df.to_csv(None, index=True)  # Replace None with the correct file name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** In this task, you will preprocess Yelp reviews and calculate the Cosine distances between them based on their term frequencies. Here's what you need to do:\n",
    "\n",
    "1. **Preprocess the Text**: Write a function to clean up the review text by removing punctuation, converting it to lowercase, tokenizing it into individual words, and removing stopwords. You’ll also lemmatize the words to reduce them to their base forms.\n",
    "\n",
    "2. **Apply Preprocessing**: Apply the text preprocessing function to the reviews in the dataset, so the text is cleaned and ready for analysis.\n",
    "\n",
    "3. **Convert to Term Matrix**: Convert the cleaned review text into a numerical format (term frequency matrix) using `CountVectorizer`. This step will help us compare the text data between different reviews.\n",
    "\n",
    "4. **Calculate Cosine Similarities**: Use the term matrix to calculate the Cosine similarity between each pair of reviews. This measures how similar the reviews are based on their word usage.\n",
    "\n",
    "5. **Convert Similarities to Distances**: Convert the Cosine similarities into Cosine distances (1 - Cosine Similarity). Distances will help you see how far apart the reviews are from one another.\n",
    "\n",
    "6. **Create and Save a DataFrame**: Convert the distance results into a DataFrame so it’s easier to read, and save it as a CSV file.\n",
    "\n",
    "The goal is to preprocess the review text and then measure how similar or different the reviews are using Cosine distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete Text Preprocessing and Cosine Distance Calculation Script for Students\n",
    "\n",
    "# TODO: Import necessary libraries (Hint: You'll need nltk for lemmatization and stop words, pandas for data handling, re for regular expressions, and sklearn for vectorization and similarity calculations)\n",
    "\n",
    "# Step 1: Initialize the lemmatizer and stop words\n",
    "lemmatizer = None  # TODO: Initialize WordNetLemmatizer\n",
    "stop_words = None  # TODO: Get the set of English stop words using nltk\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, removing stopwords, lemmatizing, and cleaning punctuation.\n",
    "    \"\"\"\n",
    "    # TODO: Remove special characters (Hint: Use re.sub to remove anything that's not a letter or space)\n",
    "    text = None  # Replace None with text cleaning code\n",
    "\n",
    "    # TODO: Convert the text to lowercase\n",
    "    text = None  # Replace None with code to convert text to lowercase\n",
    "\n",
    "    # TODO: Tokenize the text (Hint: Use word_tokenize from nltk)\n",
    "    tokens = None  # Replace None with tokenization code\n",
    "\n",
    "    # TODO: Remove stopwords and lemmatize the tokens\n",
    "    tokens = None  # Replace None with list comprehension for lemmatization and stopword removal\n",
    "    \n",
    "    # Return the processed text as a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# TODO: Load the dataset (Hint: Use pandas to read 'yelp_reviews.csv')\n",
    "data = None  # Replace None with code to load the dataset\n",
    "\n",
    "# Step 3: Apply text preprocessing to the 'Review' column\n",
    "# TODO: Use the apply method to apply preprocess_text to each review\n",
    "data['Cleaned_Reviews'] = None  # Replace None with the code to apply preprocessing\n",
    "\n",
    "# Step 4: Convert documents to a term frequency matrix using CountVectorizer\n",
    "# TODO: Initialize the CountVectorizer and fit it to the 'Cleaned_Reviews' column\n",
    "vectorizer = None  # Replace None with CountVectorizer initialization\n",
    "term_matrix = None  # Replace None with the fit_transform code for the Cleaned_Reviews\n",
    "\n",
    "# Step 5: Calculate the pairwise Cosine similarities between documents\n",
    "# TODO: Use cosine_similarity from sklearn to calculate the similarity between the term frequency vectors\n",
    "cosine_similarities = None  # Replace None with the similarity calculation code\n",
    "\n",
    "# Step 6: Convert Cosine similarities to Cosine distances (1 - Cosine Similarity)\n",
    "# TODO: Subtract the cosine similarities from 1 to get distances\n",
    "cosine_distances = None  # Replace None with the code to calculate cosine distances\n",
    "\n",
    "# Step 7: Convert the pairwise distances into a matrix format and then to a DataFrame for easier viewing\n",
    "# TODO: Create a DataFrame from the cosine distances matrix (Hint: Use pandas DataFrame and set the index and columns to data.index)\n",
    "cosine_distance_df = None  # Replace None with DataFrame creation code\n",
    "\n",
    "# Step 8: Save the DataFrame to a CSV file\n",
    "# TODO: Save the DataFrame to a CSV file (Hint: Use to_csv method)\n",
    "cosine_distance_df.to_csv(None, index=True)  # Replace None with the correct file name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** In this task, you will be using a k-NN (k-Nearest Neighbors) model to classify the sentiment of Yelp reviews as either positive or negative. Here's what you need to do:\n",
    "\n",
    "1. **Load the Data**: Load the Yelp reviews dataset from a CSV file into a DataFrame so you can work with it.\n",
    "\n",
    "2. **Convert Ratings to Sentiment**: Convert the ratings into binary sentiment labels (0 for negative, 1 for positive). You will define positive reviews as those with a rating of 3.5 or higher.\n",
    "\n",
    "3. **Preprocess the Text**: Write a function to clean the review text by converting it to lowercase, removing punctuation, and getting rid of numbers.\n",
    "\n",
    "4. **Vectorize the Reviews**: Convert the cleaned review text into a numerical format using `CountVectorizer`, which turns the text into a set of features based on word frequencies.\n",
    "\n",
    "5. **Split the Data**: Divide the dataset into training and testing sets (80% for training, 20% for testing). This will help you train the model and evaluate its performance.\n",
    "\n",
    "6. **Train the k-NN Model**: Set up the k-NN classifier with 5 neighbors and cosine similarity, and train it using the training data.\n",
    "\n",
    "7. **Evaluate the Model**: Use the model to predict sentiment on the test set and then evaluate how well it performed using accuracy score and a classification report (which shows precision, recall, and F1-score).\n",
    "\n",
    "8. **Predict a New Review**: Finally, test your model by predicting the sentiment of a new review (e.g., \"The product was absolutely terrible\").\n",
    "\n",
    "The goal is to classify the sentiment of reviews using a machine learning model and evaluate its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete k-NN Sentiment Classification Script for Students\n",
    "\n",
    "# TODO: Import necessary libraries (Hint: You'll need pandas for data handling, re for regular expressions, and sklearn for machine learning functions)\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "# TODO: Use pandas to load the 'yelp_reviews.csv' file into a DataFrame\n",
    "reviews_df = None  # Replace None with the code to load the dataset\n",
    "\n",
    "# Step 2: Convert ratings to binary sentiment (0 for < 3.5, 1 for >= 3.5)\n",
    "# TODO: Use apply to create a new column 'Sentiment' based on the 'Rating' column\n",
    "reviews_df['Sentiment'] = None  # Replace None with the lambda function for binary sentiment classification\n",
    "\n",
    "# Step 3: Preprocess the text data (cleaning reviews)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by converting to lowercase, removing punctuation, and removing numbers.\n",
    "    \"\"\"\n",
    "    # TODO: Convert text to lowercase\n",
    "    text = None  # Replace None with the code to convert text to lowercase\n",
    "    \n",
    "    # TODO: Remove punctuation (Hint: Use re.sub to remove non-alphanumeric characters)\n",
    "    text = None  # Replace None with the regex for removing punctuation\n",
    "    \n",
    "    # TODO: Remove numbers\n",
    "    text = None  # Replace None with the regex for removing numbers\n",
    "    \n",
    "    return text\n",
    "\n",
    "# TODO: Apply preprocessing to the 'Review' column (Hint: Use apply method to call preprocess_text for each review)\n",
    "reviews_df['Cleaned_Review'] = None  # Replace None with the code to apply text preprocessing\n",
    "\n",
    "# Step 4: Vectorize the cleaned reviews using CountVectorizer\n",
    "# TODO: Initialize CountVectorizer (Hint: Use stop_words='english' to remove common stopwords)\n",
    "vectorizer = None  # Replace None with CountVectorizer initialization\n",
    "\n",
    "# TODO: Fit the vectorizer to the 'Cleaned_Review' column and transform the reviews into a feature matrix\n",
    "X = None  # Replace None with the code to vectorize the cleaned reviews\n",
    "y = reviews_df['Sentiment']  # Target variable (Sentiment)\n",
    "\n",
    "# Step 5: Split the data into training and test sets (80% train, 20% test)\n",
    "# TODO: Use train_test_split to split X and y into training and test sets (Hint: Set test_size=0.2 and random_state=42)\n",
    "X_train, X_test, y_train, y_test = None  # Replace None with the train_test_split code\n",
    "\n",
    "# Step 6: Apply k-NN model\n",
    "# TODO: Initialize and train a k-NN classifier (Hint: Set n_neighbors=5 and metric='cosine')\n",
    "knn = None  # Replace None with the k-NN initialization and training code\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "# TODO: Predict the sentiment on the test set\n",
    "y_pred = None  # Replace None with the code to predict on X_test\n",
    "\n",
    "# TODO: Generate a classification report and accuracy score (Hint: Use classification_report and accuracy_score from sklearn)\n",
    "classification_report_output = None  # Replace None with the code to generate the classification report\n",
    "accuracy = None  # Replace None with the code to calculate accuracy\n",
    "\n",
    "# TODO: Print the classification report and accuracy\n",
    "print(\"Classification Report:\\n\", None)  # Replace None with classification report variable\n",
    "print(f\"Accuracy: {None}\")  # Replace None with accuracy variable\n",
    "\n",
    "# Step 8: Predict sentiment for a new document\n",
    "new_document = [\"The product was absolutely terrible\"]\n",
    "\n",
    "# TODO: Vectorize the new document using the same vectorizer\n",
    "new_doc_vector = None  # Replace None with the code to vectorize the new document\n",
    "\n",
    "# TODO: Predict the sentiment of the new document\n",
    "predicted_sentiment = None  # Replace None with the code to predict sentiment of new document\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
