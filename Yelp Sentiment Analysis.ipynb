{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** sentiment analysis on Yelp reviews using the VADER sentiment analyzer. \n",
    "The goal is to analyze Yelp reviews for sentiment and store the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-01T19:55:07.810017Z"
    }
   },
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Load the Yelp reviews dataset \n",
    "reviews_df = pd.read_csv(\"/Users/kaykaydaou/Desktop/MMA/MMA WINTER 25/W1 TEXT ANALYTICS/Labs/lab 3/yelp_reviews.csv\")\n",
    "\n",
    "#Initialize VADER sentiment analyzer \n",
    "sia = SentimentIntensityAnalyzer() \n",
    "\n",
    "#Function to calculate VADER sentiment for each review\n",
    "def get_vader_sentiment(review):\n",
    "    #Calculate sentiment using sia.polarity_scores(review)\n",
    "    #Return the positive, negative, neutral, and compound sentiment scores\n",
    "    sentiment = sia.polarity_scores(review)  \n",
    "    return sentiment['pos'], sentiment['neg'], sentiment['neu'], sentiment['compound']  \n",
    "\n",
    "#Apply the VADER sentiment analysis to the 'Review' column\n",
    "#Using the DataFrame's 'apply' function to apply get_vader_sentiment to each review, then unpacking the scores into new columns ('pos', 'neg', 'neu', 'compound')\n",
    "reviews_df['pos'], reviews_df['neg'], reviews_df['neu'], reviews_df['compound'] = reviews_df['Review'].apply(lambda x: pd.Series(get_vader_sentiment(x))) \n",
    "\n",
    "#Save the updated DataFrame with the VADER sentiment scores to a CSV file \n",
    "# Ensure the file is named 'yelp_reviews_vader.csv'\n",
    "reviews_df.to_csv(\"yelp_reviews_vader.csv\", index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Task 2:** preprocessing text and performing sentiment analysis using custom rules and WordNet similarity. The goal is to preprocess the text, extract meaningful phrases, and determine the sentiment of each review using custom rules and word similarity.\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:49:38.211822Z",
     "start_time": "2025-02-01T21:47:40.655386Z"
    }
   },
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Initialize the lemmatizer and stop words list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    #Tokenize the text \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #Convert tokens to lowercase and remove non-alphanumeric characters\n",
    "    #Filter out stop words and lemmatize the remaining tokens\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def extract_phrases(tokens):\n",
    "    \"\"\"\n",
    "    Extracts phrases from the tokens based on the provided rules.\n",
    "    \"\"\"\n",
    "    #POS-tag the tokens \n",
    "    pos_tagged = pos_tag(tokens)  \n",
    "    phrases = []\n",
    "    \n",
    "    #Implement rules to extract phrases (use the given rules 1-5 as hints)\n",
    "    for i in range(len(pos_tagged) - 2):\n",
    "        word1, tag1 = pos_tagged[i]\n",
    "        word2, tag2 = pos_tagged[i + 1]\n",
    "        word3, tag3 = pos_tagged[i + 2] if i + 2 < len(pos_tagged) else None\n",
    "\n",
    "        # Rule 1: JJ followed by NN or NNS\n",
    "        if tag1.startswith('JJ') and (tag2 == 'NN' or tag2 == 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 2: RB, RBR, or RBS followed by JJ, not followed by NN or NNS\n",
    "        elif (tag1 == 'RB' or tag1 == 'RBR' or tag1 == 'RBS') and tag2.startswith('JJ') and (tag3 != 'NN' and tag3 != 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 3: JJ followed by JJ, not followed by NN or NNS\n",
    "        elif tag1.startswith('JJ') and tag2.startswith('JJ') and (tag3 != 'NN' and tag3 != 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 4: NN or NNS followed by JJ, not followed by NN or NNS\n",
    "        elif (tag1 == 'NN' or tag1 == 'NNS') and tag2.startswith('JJ') and (tag3 != 'NN' and tag3 != 'NNS'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "        \n",
    "        # Rule 5: RB, RBR, or RBS followed by VB, VBD, VBN, or VBG\n",
    "        elif (tag1 == 'RB' or tag1 == 'RBR' or tag1 == 'RBS') and (tag2 == 'VB' or tag2 == 'VBD' or tag2 == 'VBN' or tag2 == 'VBG'):\n",
    "            phrases.append(f'{word1} {word2}')\n",
    "\n",
    "    return phrases\n",
    "\n",
    "def wordnet_similarity(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two words using WordNet.\n",
    "    \"\"\"\n",
    "    #Get synsets for the two words using wn.synsets\n",
    "    synsets1 = wn.synsets(word1)  \n",
    "    synsets2 = wn.synsets(word2)  \n",
    "\n",
    "    if synsets1 and synsets2:\n",
    "        #Calculate similarity using wn.wup_similarity\n",
    "        return max((s1.wup_similarity(s2) or 0) for s1 in synsets1 for s2 in synsets2)\n",
    "\n",
    "    return 0  # Return 0 if no similarity is found\n",
    "\n",
    "# Define positive and negative reference words\n",
    "positive_refs = [\"delicious\", \"tasty\", \"amazing\", \"great\", \"wonderful\", \"fantastic\", \"excellent\"]\n",
    "negative_refs = [\"disgusting\", \"bad\", \"terrible\", \"awful\", \"horrible\", \"inedible\", \"poor\"]\n",
    "\n",
    "def semantic_orientation(phrase):\n",
    "    \"\"\"\n",
    "    Calculates the semantic orientation of a phrase by comparing it with\n",
    "    multiple positive and negative reference words.\n",
    "    \"\"\"\n",
    "    #Calculate the average similarity with positive and negative reference words\n",
    "    words = phrase.split()\n",
    "    pos_score = sum(max(wordnet_similarity(word, ref) for ref in positive_refs) for word in words)\n",
    "    neg_score = neg_score = sum(max(wordnet_similarity(word, ref) for ref in negative_refs) for word in words)\n",
    "    # Return the difference to get the orientation score\n",
    "    return pos_score - neg_score\n",
    "\n",
    "def analyze_sentiment(document):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of a document based on its phrases' semantic orientation\n",
    "    and returns both the sentiment score and the sentiment label.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(document)\n",
    "    phrases = extract_phrases(tokens)\n",
    "    total_orientation = 0\n",
    "\n",
    "    for phrase in phrases:\n",
    "        total_orientation += semantic_orientation(phrase)\n",
    "\n",
    "    # TODO: Assign sentiment label based on total_orientation\n",
    "    sentiment_label = None  # Replace None with logic for assigning 'Positive' or 'Negative' label\n",
    "\n",
    "    return total_orientation, sentiment_label  # Return both the sentiment score and label\n",
    "\n",
    "#Load the dataset \n",
    "data = pd.read_csv(\"/Users/kaykaydaou/Desktop/MMA/MMA WINTER 25/W1 TEXT ANALYTICS/Labs/lab 3/yelp_reviews.csv\")\n",
    "\n",
    "#Apply sentiment analysis to the 'Review' column and calculate both the score and label\n",
    "#Use the apply method with a lambda function to apply analyze_sentiment to each review\n",
    "data[['Sentiment Score', 'Sentiment Label']] = data['Review'].apply(lambda x: pd.Series(analyze_sentiment(x)))  \n",
    "\n",
    "# Save the results to a new CSV file \n",
    "data.to_csv(\"yelp_reviews_custom_sentiment.csv\", index=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** preprocess Yelp reviews and calculate the Euclidean distances between them based on their term frequencies. \n",
    "The goal is to process the text data and then compute how close or far apart the reviews are from each other based on the words they contain.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:53:43.808342Z",
     "start_time": "2025-02-01T21:53:43.342286Z"
    }
   },
   "source": [
    "#Import necessary libraries \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Step 1: Initialize the lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, removing stopwords, lemmatizing, and cleaning punctuation.\n",
    "    \"\"\"\n",
    "    #Remove special characters (Use re.sub to remove anything that's not a letter or space)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "#Load the dataset \n",
    "data = pd.read_csv(\"/Users/kaykaydaou/Desktop/MMA/MMA WINTER 25/W1 TEXT ANALYTICS/Labs/lab 3/yelp_reviews.csv\")\n",
    "\n",
    "# Step 3: Apply text preprocessing to the 'Review' column\n",
    "data['Cleaned_Reviews'] = data['Review'].apply(preprocess_text) \n",
    "\n",
    "# Step 4: Convert documents to a term frequency matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_matrix = vectorizer.fit_transform(data['Cleaned_Reviews'])\n",
    "\n",
    "# Step 5: Calculate the pairwise Euclidean distances between documents\n",
    "distances = euclidean_distances(term_matrix)\n",
    "\n",
    "# Step 6: Convert the pairwise distances into a matrix format\n",
    "# Step 7: Convert the matrix into a DataFrame for easier viewing\n",
    "distance_df = pd.DataFrame(distances, index=data.index, columns=data.index)\n",
    "\n",
    "# Step 8: Save the DataFrame to a CSV file\n",
    "distance_df.to_csv(\"yelp_reviews_euclidean_distances.csv\", index=True)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** preprocess Yelp reviews and calculate the Cosine distances between them based on their term frequencies.\n",
    "The goal is to preprocess the review text and then measure how similar or different the reviews are using Cosine distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:57:45.929626Z",
     "start_time": "2025-02-01T21:57:45.445924Z"
    }
   },
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Initialize the lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, removing stopwords, lemmatizing, and cleaning punctuation.\n",
    "    \"\"\"\n",
    "    #Remove special characters \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Remove stopwords and lemmatize\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#load the dataset\n",
    "data = pd.read_csv(\"/Users/kaykaydaou/Desktop/MMA/MMA WINTER 25/W1 TEXT ANALYTICS/Labs/lab 3/yelp_reviews.csv\")\n",
    "\n",
    "# Step 3: Apply text preprocessing to the 'Review' column\n",
    "data['Cleaned_Reviews'] = data['Review'].apply(preprocess_text)\n",
    "\n",
    "# Step 4: Convert documents to a term frequency matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "term_matrix = vectorizer.fit_transform(data['Cleaned_Reviews'])\n",
    "\n",
    "# Step 5: Calculate the pairwise Cosine similarities between documents\n",
    "cosine_similarities = cosine_similarity(term_matrix) \n",
    "\n",
    "# Step 6: Convert Cosine similarities to Cosine distances (1 - Cosine Similarity)\n",
    "cosine_distances = 1 - cosine_similarities\n",
    "\n",
    "# Step 7: Convert the pairwise distances into a matrix format and then to a DataFrame for easier viewing\n",
    "cosine_distance_df = pd.DataFrame(cosine_distances, index=data.index, columns=data.index)\n",
    "\n",
    "# Step 8: Save the DataFrame to a CSV file\n",
    "cosine_distance_df.to_csv(\"yelp_reviews_cosine_distances.csv\", index=True)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** k-NN (k-Nearest Neighbors) model to classify the sentiment of Yelp reviews as either positive or negative. \n",
    "The goal is to classify the sentiment of reviews using a machine learning model and evaluate its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incomplete k-NN Sentiment Classification Script for Students\n",
    "\n",
    "# TODO: Import necessary libraries (Hint: You'll need pandas for data handling, re for regular expressions, and sklearn for machine learning functions)\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "# TODO: Use pandas to load the 'yelp_reviews.csv' file into a DataFrame\n",
    "reviews_df = None  # Replace None with the code to load the dataset\n",
    "\n",
    "# Step 2: Convert ratings to binary sentiment (0 for < 3.5, 1 for >= 3.5)\n",
    "# TODO: Use apply to create a new column 'Sentiment' based on the 'Rating' column\n",
    "reviews_df['Sentiment'] = None  # Replace None with the lambda function for binary sentiment classification\n",
    "\n",
    "# Step 3: Preprocess the text data (cleaning reviews)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by converting to lowercase, removing punctuation, and removing numbers.\n",
    "    \"\"\"\n",
    "    # TODO: Convert text to lowercase\n",
    "    text = None  # Replace None with the code to convert text to lowercase\n",
    "    \n",
    "    # TODO: Remove punctuation (Hint: Use re.sub to remove non-alphanumeric characters)\n",
    "    text = None  # Replace None with the regex for removing punctuation\n",
    "    \n",
    "    # TODO: Remove numbers\n",
    "    text = None  # Replace None with the regex for removing numbers\n",
    "    \n",
    "    return text\n",
    "\n",
    "# TODO: Apply preprocessing to the 'Review' column (Hint: Use apply method to call preprocess_text for each review)\n",
    "reviews_df['Cleaned_Review'] = None  # Replace None with the code to apply text preprocessing\n",
    "\n",
    "# Step 4: Vectorize the cleaned reviews using CountVectorizer\n",
    "# TODO: Initialize CountVectorizer (Hint: Use stop_words='english' to remove common stopwords)\n",
    "vectorizer = None  # Replace None with CountVectorizer initialization\n",
    "\n",
    "# TODO: Fit the vectorizer to the 'Cleaned_Review' column and transform the reviews into a feature matrix\n",
    "X = None  # Replace None with the code to vectorize the cleaned reviews\n",
    "y = reviews_df['Sentiment']  # Target variable (Sentiment)\n",
    "\n",
    "# Step 5: Split the data into training and test sets (80% train, 20% test)\n",
    "# TODO: Use train_test_split to split X and y into training and test sets (Hint: Set test_size=0.2 and random_state=42)\n",
    "X_train, X_test, y_train, y_test = None  # Replace None with the train_test_split code\n",
    "\n",
    "# Step 6: Apply k-NN model\n",
    "# TODO: Initialize and train a k-NN classifier (Hint: Set n_neighbors=5 and metric='cosine')\n",
    "knn = None  # Replace None with the k-NN initialization and training code\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "# TODO: Predict the sentiment on the test set\n",
    "y_pred = None  # Replace None with the code to predict on X_test\n",
    "\n",
    "# TODO: Generate a classification report and accuracy score (Hint: Use classification_report and accuracy_score from sklearn)\n",
    "classification_report_output = None  # Replace None with the code to generate the classification report\n",
    "accuracy = None  # Replace None with the code to calculate accuracy\n",
    "\n",
    "# TODO: Print the classification report and accuracy\n",
    "print(\"Classification Report:\\n\", None)  # Replace None with classification report variable\n",
    "print(f\"Accuracy: {None}\")  # Replace None with accuracy variable\n",
    "\n",
    "# Step 8: Predict sentiment for a new document\n",
    "new_document = [\"The product was absolutely terrible\"]\n",
    "\n",
    "# TODO: Vectorize the new document using the same vectorizer\n",
    "new_doc_vector = None  # Replace None with the code to vectorize the new document\n",
    "\n",
    "# TODO: Predict the sentiment of the new document\n",
    "predicted_sentiment = None  # Replace None with the code to predict sentiment of new document\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
